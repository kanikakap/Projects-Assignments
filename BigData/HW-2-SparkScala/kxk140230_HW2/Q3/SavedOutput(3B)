Last login: Fri Oct 16 00:39:35 on ttys000
kanikas-MacBook-Pro:~ kanikakapoor$ ssh kxk140230@cs6360.utdallas.edu

                 Department of Computer Science
                 University of Texas at Dallas

  Pursuant to Texas Administrative Code 202:

  (1) Unauthorized use is prohibited;

  (2) Usage may be subject to security testing and monitoring;

  (3) Misuse is subject to criminal prosecution; and

  (4) No expectation of privacy except as otherwise provided by
      applicable privacy laws.


kxk140230@cs6360.utdallas.edu's password: 
Last login: Fri Oct 16 00:39:49 2015 from 10.21.74.218
Sourcing /usr/local/etc/skel/global/profile
-bash: rt: command not found
{cs6360:~} spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/10/16 00:49:15 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 00:49:15 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 00:49:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 00:49:15 INFO spark.HttpServer: Starting HTTP Server
15/10/16 00:49:15 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:15 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:36564
15/10/16 00:49:15 INFO util.Utils: Successfully started service 'HTTP class server' on port 36564.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/16 00:49:23 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 00:49:23 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 00:49:23 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 00:49:24 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/10/16 00:49:24 INFO Remoting: Starting remoting
15/10/16 00:49:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@cs6360.utdallas.edu:54307]
15/10/16 00:49:25 INFO util.Utils: Successfully started service 'sparkDriver' on port 54307.
15/10/16 00:49:25 INFO spark.SparkEnv: Registering MapOutputTracker
15/10/16 00:49:25 INFO spark.SparkEnv: Registering BlockManagerMaster
15/10/16 00:49:25 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20151016004925-e0b5
15/10/16 00:49:25 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/10/16 00:49:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/16 00:49:26 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-df82b312-45ef-44b3-8ec7-30ef3de71ed8
15/10/16 00:49:26 INFO spark.HttpServer: Starting HTTP Server
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:40203
15/10/16 00:49:26 INFO util.Utils: Successfully started service 'HTTP file server' on port 40203.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@67a60374: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@54bedfe1: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4042: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@632796c6: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4043: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@5b81e58: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4044: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@552080ff: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4045: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@6080cbf6: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
15/10/16 00:49:26 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4046: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@2922db62: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:26 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4047: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@302569cc: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4048: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@51c07d70: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4049: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@210e5887: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4050: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@3477c504: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4051: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@64a4b7c6: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4052: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@752f88f0: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:49:27 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:49:27 WARN util.Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.
15/10/16 00:49:27 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:49:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4053
15/10/16 00:49:27 INFO util.Utils: Successfully started service 'SparkUI' on port 4053.
15/10/16 00:49:27 INFO ui.SparkUI: Started SparkUI at http://cs6360.utdallas.edu:4053
15/10/16 00:49:27 INFO executor.Executor: Using REPL class URI: http://10.176.92.90:36564
15/10/16 00:49:27 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@cs6360.utdallas.edu:54307/user/HeartbeatReceiver
15/10/16 00:49:28 INFO netty.NettyBlockTransferService: Server created on 58976
15/10/16 00:49:28 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/10/16 00:49:28 INFO storage.BlockManagerMasterActor: Registering block manager localhost:58976 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 58976)
15/10/16 00:49:28 INFO storage.BlockManagerMaster: Registered BlockManager
15/10/16 00:49:28 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.

scala> var businessFile= sc.textFile("/yelpdatafall/business/business.csv")
15/10/16 00:57:04 INFO storage.MemoryStore: ensureFreeSpace(166036) called with curMem=0, maxMem=278302556
15/10/16 00:57:04 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 162.1 KB, free 265.3 MB)
15/10/16 00:57:04 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=166036, maxMem=278302556
15/10/16 00:57:04 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.2 MB)
15/10/16 00:57:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:58976 (size: 22.7 KB, free: 265.4 MB)
15/10/16 00:57:04 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/10/16 00:57:04 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:12
businessFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/business/business.csv MappedRDD[1] at textFile at <console>:12

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
<console>:2: error: ')' expected but 'var' found.
       var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
       ^

scala> var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
<console>:10: error: not found: value reviewFile
       var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
                           ^

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     |      
     | val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
     | val rate = ab.map(l => (l._2, l._1)). top(10).map(l=>(l._2,l._1))
     | var finalOP1 = sc.parallelize(rate)
     | businessFileMap.join(finalOP1).collect().foreach(println)
     | 
     | exit
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> 

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     |      }
myAvgFunc: [T](a: Iterable[T])(implicit num: Numeric[T])Double

scala> val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
<console>:12: error: not found: value reviewFileMap
       val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
                ^

scala> var businessFile= sc.textFile("/yelpdatafall/business/business.csv")
15/10/16 00:58:07 INFO storage.MemoryStore: ensureFreeSpace(166084) called with curMem=189325, maxMem=278302556
15/10/16 00:58:07 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 162.2 KB, free 265.1 MB)
15/10/16 00:58:07 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=355409, maxMem=278302556
15/10/16 00:58:07 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.0 MB)
15/10/16 00:58:07 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:58976 (size: 22.7 KB, free: 265.4 MB)
15/10/16 00:58:07 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/10/16 00:58:07 INFO spark.SparkContext: Created broadcast 1 from textFile at <console>:12
businessFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/business/business.csv MappedRDD[3] at textFile at <console>:12

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
{cs6360:~} clear

{cs6360:~} spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/10/16 00:58:32 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 00:58:32 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 00:58:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 00:58:32 INFO spark.HttpServer: Starting HTTP Server
15/10/16 00:58:32 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:58:32 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:49701
15/10/16 00:58:32 INFO util.Utils: Successfully started service 'HTTP class server' on port 49701.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/16 00:58:39 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 00:58:39 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 00:58:39 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 00:58:39 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/10/16 00:58:40 INFO Remoting: Starting remoting
15/10/16 00:58:40 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@cs6360.utdallas.edu:36001]
15/10/16 00:58:40 INFO util.Utils: Successfully started service 'sparkDriver' on port 36001.
15/10/16 00:58:40 INFO spark.SparkEnv: Registering MapOutputTracker
15/10/16 00:58:40 INFO spark.SparkEnv: Registering BlockManagerMaster
15/10/16 00:58:40 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20151016005840-d202
15/10/16 00:58:40 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/10/16 00:58:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/16 00:58:40 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-2d3b8cb4-7177-447f-9e26-97553f98a9fe
15/10/16 00:58:40 INFO spark.HttpServer: Starting HTTP Server
15/10/16 00:58:40 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:58:40 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:48379
15/10/16 00:58:40 INFO util.Utils: Successfully started service 'HTTP file server' on port 48379.
15/10/16 00:58:40 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:58:40 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:58:40 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@7a9373c8: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:58:40 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:58:40 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/10/16 00:58:40 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:58:41 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:58:41 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@440b92a3: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 00:58:41 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 00:58:41 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
15/10/16 00:58:41 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 00:58:41 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4042
15/10/16 00:58:41 INFO util.Utils: Successfully started service 'SparkUI' on port 4042.
15/10/16 00:58:41 INFO ui.SparkUI: Started SparkUI at http://cs6360.utdallas.edu:4042
15/10/16 00:58:41 INFO executor.Executor: Using REPL class URI: http://10.176.92.90:49701
15/10/16 00:58:41 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@cs6360.utdallas.edu:36001/user/HeartbeatReceiver
15/10/16 00:58:41 INFO netty.NettyBlockTransferService: Server created on 46338
15/10/16 00:58:41 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/10/16 00:58:41 INFO storage.BlockManagerMasterActor: Registering block manager localhost:46338 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 46338)
15/10/16 00:58:41 INFO storage.BlockManagerMaster: Registered BlockManager
15/10/16 00:58:42 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.

scala> var businessFile= sc.textFile("/yelpdatafall/business/business.csv")
15/10/16 00:58:48 INFO storage.MemoryStore: ensureFreeSpace(166036) called with curMem=0, maxMem=278302556
15/10/16 00:58:48 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 162.1 KB, free 265.3 MB)
15/10/16 00:58:48 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=166036, maxMem=278302556
15/10/16 00:58:48 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.2 MB)
15/10/16 00:58:48 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:46338 (size: 22.7 KB, free: 265.4 MB)
15/10/16 00:58:48 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/10/16 00:58:48 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:12
businessFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/business/business.csv MappedRDD[1] at textFile at <console>:12

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | \
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> var businessFile= sc.textFile("/yelpdatafall/business/business.csv")
15/10/16 01:00:02 INFO storage.MemoryStore: ensureFreeSpace(166084) called with curMem=189325, maxMem=278302556
15/10/16 01:00:02 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 162.2 KB, free 265.1 MB)
15/10/16 01:00:02 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=355409, maxMem=278302556
15/10/16 01:00:02 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.0 MB)
15/10/16 01:00:02 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:46338 (size: 22.7 KB, free: 265.4 MB)
15/10/16 01:00:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/10/16 01:00:02 INFO spark.SparkContext: Created broadcast 1 from textFile at <console>:12
businessFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/business/business.csv MappedRDD[3] at textFile at <console>:12

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
<console>:10: error: not found: value reviewFile
       var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
                           ^

scala> var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
15/10/16 01:00:43 INFO storage.MemoryStore: ensureFreeSpace(166084) called with curMem=378698, maxMem=278302556
15/10/16 01:00:43 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 162.2 KB, free 264.9 MB)
15/10/16 01:00:43 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=544782, maxMem=278302556
15/10/16 01:00:43 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.7 KB, free 264.9 MB)
15/10/16 01:00:43 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:46338 (size: 22.7 KB, free: 265.3 MB)
15/10/16 01:00:43 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/10/16 01:00:43 INFO spark.SparkContext: Created broadcast 2 from textFile at <console>:12
reviewFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/review/review.csv MappedRDD[5] at textFile at <console>:12

scala> var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
15/10/16 01:00:54 INFO mapred.FileInputFormat: Total input paths to process : 1
reviewFileMap: org.apache.spark.rdd.RDD[(String, Iterable[Double])] = ShuffledRDD[8] at groupByKey at <console>:14

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     |      } 
myAvgFunc: [T](a: Iterable[T])(implicit num: Numeric[T])Double

scala> val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
15/10/16 01:01:11 INFO spark.SparkContext: Starting job: sortByKey at <console>:18
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Registering RDD 7 (map at <console>:14)
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Got job 0 (sortByKey at <console>:18) with 2 output partitions (allowLocal=false)
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Final stage: Stage 1(sortByKey at <console>:18)
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 0)
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Missing parents: List(Stage 0)
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[7] at map at <console>:14), which has no missing parents
15/10/16 01:01:11 INFO storage.MemoryStore: ensureFreeSpace(3568) called with curMem=568071, maxMem=278302556
15/10/16 01:01:11 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 264.9 MB)
15/10/16 01:01:11 INFO storage.MemoryStore: ensureFreeSpace(2523) called with curMem=571639, maxMem=278302556
15/10/16 01:01:11 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 264.9 MB)
15/10/16 01:01:11 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:46338 (size: 2.5 KB, free: 265.3 MB)
15/10/16 01:01:11 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/10/16 01:01:11 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/10/16 01:01:11 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[7] at map at <console>:14)
15/10/16 01:01:11 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/10/16 01:01:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1300 bytes)
15/10/16 01:01:11 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1300 bytes)
15/10/16 01:01:11 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/16 01:01:11 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
15/10/16 01:01:11 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:12047591+12047592
15/10/16 01:01:11 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:0+12047591
15/10/16 01:01:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/10/16 01:01:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/10/16 01:01:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/10/16 01:01:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/10/16 01:01:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/10/16 01:01:13 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1896 bytes result sent to driver
15/10/16 01:01:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1908 ms on localhost (1/2)
15/10/16 01:01:13 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1896 bytes result sent to driver
15/10/16 01:01:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1991 ms on localhost (2/2)
15/10/16 01:01:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/16 01:01:13 INFO scheduler.DAGScheduler: Stage 0 (map at <console>:14) finished in 2.017 s
15/10/16 01:01:13 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:01:13 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:01:13 INFO scheduler.DAGScheduler: waiting: Set(Stage 1)
15/10/16 01:01:13 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:01:13 INFO scheduler.DAGScheduler: Missing parents for Stage 1: List()
15/10/16 01:01:13 INFO scheduler.DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[11] at sortByKey at <console>:18), which is now runnable
15/10/16 01:01:13 INFO storage.MemoryStore: ensureFreeSpace(6456) called with curMem=574162, maxMem=278302556
15/10/16 01:01:13 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.3 KB, free 264.9 MB)
15/10/16 01:01:13 INFO storage.MemoryStore: ensureFreeSpace(4203) called with curMem=580618, maxMem=278302556
15/10/16 01:01:13 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.9 MB)
15/10/16 01:01:13 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:46338 (size: 4.1 KB, free: 265.3 MB)
15/10/16 01:01:13 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/10/16 01:01:13 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:838
15/10/16 01:01:13 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[11] at sortByKey at <console>:18)
15/10/16 01:01:13 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/10/16 01:01:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:01:13 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:01:13 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
15/10/16 01:01:13 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
15/10/16 01:01:13 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:13 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/10/16 01:01:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/10/16 01:01:15 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2530 bytes result sent to driver
15/10/16 01:01:15 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2530 bytes result sent to driver
15/10/16 01:01:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1713 ms on localhost (1/2)
15/10/16 01:01:15 INFO scheduler.DAGScheduler: Stage 1 (sortByKey at <console>:18) finished in 1.721 s
15/10/16 01:01:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1719 ms on localhost (2/2)
15/10/16 01:01:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/10/16 01:01:15 INFO scheduler.DAGScheduler: Job 0 finished: sortByKey at <console>:18, took 3.892737 s
ab: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[12] at sortByKey at <console>:18

scala> val rate = ab.map(l => (l._2, l._1)). top(10).map(l=>(l._2,l._1))
15/10/16 01:01:22 INFO spark.SparkContext: Starting job: top at <console>:20
15/10/16 01:01:22 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Registering RDD 9 (map at <console>:18)
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Got job 1 (top at <console>:20) with 2 output partitions (allowLocal=false)
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Final stage: Stage 4(top at <console>:20)
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 3)
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Missing parents: List(Stage 3)
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Submitting Stage 3 (MappedRDD[9] at map at <console>:18), which has no missing parents
15/10/16 01:01:22 INFO storage.MemoryStore: ensureFreeSpace(6496) called with curMem=584821, maxMem=278302556
15/10/16 01:01:22 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.3 KB, free 264.8 MB)
15/10/16 01:01:22 INFO storage.MemoryStore: ensureFreeSpace(4233) called with curMem=591317, maxMem=278302556
15/10/16 01:01:22 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.8 MB)
15/10/16 01:01:22 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:46338 (size: 4.1 KB, free: 265.3 MB)
15/10/16 01:01:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/10/16 01:01:22 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:838
15/10/16 01:01:22 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 3 (MappedRDD[9] at map at <console>:18)
15/10/16 01:01:22 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
15/10/16 01:01:22 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:01:22 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:01:22 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 4)
15/10/16 01:01:22 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 5)
15/10/16 01:01:22 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:01:22 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:01:23 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1001 bytes result sent to driver
15/10/16 01:01:23 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 1735 ms on localhost (1/2)
15/10/16 01:01:23 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1001 bytes result sent to driver
15/10/16 01:01:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 1754 ms on localhost (2/2)
15/10/16 01:01:23 INFO scheduler.DAGScheduler: Stage 3 (map at <console>:18) finished in 1.751 s
15/10/16 01:01:23 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:01:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/10/16 01:01:23 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:01:23 INFO scheduler.DAGScheduler: waiting: Set(Stage 4)
15/10/16 01:01:23 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:01:23 INFO scheduler.DAGScheduler: Missing parents for Stage 4: List()
15/10/16 01:01:23 INFO scheduler.DAGScheduler: Submitting Stage 4 (MapPartitionsRDD[14] at top at <console>:20), which is now runnable
15/10/16 01:01:23 INFO storage.MemoryStore: ensureFreeSpace(3376) called with curMem=595550, maxMem=278302556
15/10/16 01:01:23 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 264.8 MB)
15/10/16 01:01:23 INFO storage.MemoryStore: ensureFreeSpace(2395) called with curMem=598926, maxMem=278302556
15/10/16 01:01:23 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 264.8 MB)
15/10/16 01:01:23 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:46338 (size: 2.3 KB, free: 265.3 MB)
15/10/16 01:01:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/10/16 01:01:23 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/10/16 01:01:23 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 4 (MapPartitionsRDD[14] at top at <console>:20)
15/10/16 01:01:23 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
15/10/16 01:01:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:01:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:01:23 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 7)
15/10/16 01:01:23 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 6)
15/10/16 01:01:23 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:01:23 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:01:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:01:24 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 7). 1833 bytes result sent to driver
15/10/16 01:01:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 291 ms on localhost (1/2)
15/10/16 01:01:24 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 6). 1833 bytes result sent to driver
15/10/16 01:01:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 318 ms on localhost (2/2)
15/10/16 01:01:24 INFO scheduler.DAGScheduler: Stage 4 (top at <console>:20) finished in 0.318 s
15/10/16 01:01:24 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/10/16 01:01:24 INFO scheduler.DAGScheduler: Job 1 finished: top at <console>:20, took 2.117953 s
rate: Array[(String, Double)] = Array((zx1lLUvlRUN5nQlSj3HRDw,5.0), (zweAzZDJ8SfwUID1UEGkbA,5.0), (zvo21oKr656PQNVblYxYlg,5.0), (ztswDToyZGyL_dxo0CEQew,5.0), (zrmO-d-Mw3kv9dWa7Trr9Q,5.0), (zqZjrcTf9Bc7r_VLGN4mrw,5.0), (zkD9AtBT9ZkLFiV31gvEGw,5.0), (ziS_fZ7Z99fa4qNVr879vg,5.0), (zh2Eja5h54vM7GUibHoi9A,5.0), (zh-MNFY4TyG7x2itUVlESQ,5.0))

scala> var finalOP1 = sc.parallelize(rate)
finalOP1: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[15] at parallelize at <console>:22

scala> businessFileMap.join(finalOP1).collect().foreach(println)
<console>:25: error: not found: value businessFileMap
              businessFileMap.join(finalOP1).collect().foreach(println)
              ^

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> var businessFileMap = businessFile.map(l => l.split("\\^")).map(l=>(l(0),(l(1),l(2)))
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> val businessFileMap = sc.textFile("/yelpdatafall/business/business.csv").map(l => l.split("\\^")).map(l => (l(0),(l(1),l(2))))
15/10/16 01:03:51 INFO storage.MemoryStore: ensureFreeSpace(81443) called with curMem=601321, maxMem=278302556
15/10/16 01:03:51 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 79.5 KB, free 264.8 MB)
15/10/16 01:03:51 INFO storage.MemoryStore: ensureFreeSpace(32041) called with curMem=682764, maxMem=278302556
15/10/16 01:03:51 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.3 KB, free 264.7 MB)
15/10/16 01:03:51 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:46338 (size: 31.3 KB, free: 265.3 MB)
15/10/16 01:03:51 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/10/16 01:03:51 INFO spark.SparkContext: Created broadcast 7 from textFile at <console>:12
businessFileMap: org.apache.spark.rdd.RDD[(String, (String, String))] = MappedRDD[19] at map at <console>:12

scala> businessFileMap.join(finalOP1).collect().foreach(println)
15/10/16 01:04:02 INFO mapred.FileInputFormat: Total input paths to process : 1
15/10/16 01:04:02 INFO spark.SparkContext: Starting job: collect at <console>:27
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Registering RDD 19 (map at <console>:12)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Registering RDD 15 (parallelize at <console>:22)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Got job 2 (collect at <console>:27) with 4 output partitions (allowLocal=false)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Final stage: Stage 7(collect at <console>:27)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 5, Stage 6)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Missing parents: List(Stage 5, Stage 6)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Submitting Stage 5 (MappedRDD[19] at map at <console>:12), which has no missing parents
15/10/16 01:04:02 INFO storage.MemoryStore: ensureFreeSpace(3240) called with curMem=714805, maxMem=278302556
15/10/16 01:04:02 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 3.2 KB, free 264.7 MB)
15/10/16 01:04:02 INFO storage.MemoryStore: ensureFreeSpace(2315) called with curMem=718045, maxMem=278302556
15/10/16 01:04:02 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.3 KB, free 264.7 MB)
15/10/16 01:04:02 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:46338 (size: 2.3 KB, free: 265.3 MB)
15/10/16 01:04:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/10/16 01:04:02 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:838
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 5 (MappedRDD[19] at map at <console>:12)
15/10/16 01:04:02 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Submitting Stage 6 (ParallelCollectionRDD[15] at parallelize at <console>:22), which has no missing parents
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 8, localhost, ANY, 1304 bytes)
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 9, localhost, ANY, 1304 bytes)
15/10/16 01:04:02 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 8)
15/10/16 01:04:02 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 9)
15/10/16 01:04:02 INFO storage.MemoryStore: ensureFreeSpace(1488) called with curMem=720360, maxMem=278302556
15/10/16 01:04:02 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1488.0 B, free 264.7 MB)
15/10/16 01:04:02 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:0+1486763
15/10/16 01:04:02 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:1486763+1486763
15/10/16 01:04:02 INFO storage.MemoryStore: ensureFreeSpace(1082) called with curMem=721848, maxMem=278302556
15/10/16 01:04:02 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1082.0 B, free 264.7 MB)
15/10/16 01:04:02 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:46338 (size: 1082.0 B, free: 265.3 MB)
15/10/16 01:04:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/10/16 01:04:02 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from Stage 6 (ParallelCollectionRDD[15] at parallelize at <console>:22)
15/10/16 01:04:02 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 4 tasks
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10, localhost, PROCESS_LOCAL, 1462 bytes)
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 11, localhost, PROCESS_LOCAL, 1507 bytes)
15/10/16 01:04:02 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 10)
15/10/16 01:04:02 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 11)
15/10/16 01:04:02 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 10). 840 bytes result sent to driver
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 12, localhost, PROCESS_LOCAL, 1462 bytes)
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 120 ms on localhost (1/4)
15/10/16 01:04:02 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 12)
15/10/16 01:04:02 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 11). 840 bytes result sent to driver
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 13, localhost, PROCESS_LOCAL, 1507 bytes)
15/10/16 01:04:02 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 12). 840 bytes result sent to driver
15/10/16 01:04:02 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 13)
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 11) in 162 ms on localhost (2/4)
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 12) in 49 ms on localhost (3/4)
15/10/16 01:04:02 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 13). 840 bytes result sent to driver
15/10/16 01:04:02 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 13) in 31 ms on localhost (4/4)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Stage 6 (parallelize at <console>:22) finished in 0.182 s
15/10/16 01:04:02 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:04:02 INFO scheduler.DAGScheduler: running: Set(Stage 5)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: waiting: Set(Stage 7)
15/10/16 01:04:02 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:04:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/10/16 01:04:02 INFO scheduler.DAGScheduler: Missing parents for Stage 7: List(Stage 5)
15/10/16 01:04:03 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 9). 1898 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 9) in 362 ms on localhost (1/2)
15/10/16 01:04:03 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 8). 1898 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 8) in 376 ms on localhost (2/2)
15/10/16 01:04:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Stage 5 (map at <console>:12) finished in 0.378 s
15/10/16 01:04:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:04:03 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:04:03 INFO scheduler.DAGScheduler: waiting: Set(Stage 7)
15/10/16 01:04:03 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Missing parents for Stage 7: List()
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Submitting Stage 7 (FlatMappedValuesRDD[22] at join at <console>:27), which is now runnable
15/10/16 01:04:03 INFO storage.MemoryStore: ensureFreeSpace(2488) called with curMem=722930, maxMem=278302556
15/10/16 01:04:03 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 2.4 KB, free 264.7 MB)
15/10/16 01:04:03 INFO storage.MemoryStore: ensureFreeSpace(1678) called with curMem=725418, maxMem=278302556
15/10/16 01:04:03 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1678.0 B, free 264.7 MB)
15/10/16 01:04:03 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:46338 (size: 1678.0 B, free: 265.3 MB)
15/10/16 01:04:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_10_piece0
15/10/16 01:04:03 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from Stage 7 (FlatMappedValuesRDD[22] at join at <console>:27)
15/10/16 01:04:03 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 4 tasks
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, localhost, PROCESS_LOCAL, 1902 bytes)
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 15, localhost, PROCESS_LOCAL, 1902 bytes)
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 16, localhost, PROCESS_LOCAL, 1902 bytes)
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 17, localhost, PROCESS_LOCAL, 1902 bytes)
15/10/16 01:04:03 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 14)
15/10/16 01:04:03 INFO executor.Executor: Running task 2.0 in stage 7.0 (TID 16)
15/10/16 01:04:03 INFO executor.Executor: Running task 3.0 in stage 7.0 (TID 17)
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 15)
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
15/10/16 01:04:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:04:03 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 15). 1450 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 15) in 460 ms on localhost (1/4)
15/10/16 01:04:03 INFO executor.Executor: Finished task 2.0 in stage 7.0 (TID 16). 1653 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 16) in 487 ms on localhost (2/4)
15/10/16 01:04:03 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 14). 820 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 516 ms on localhost (3/4)
15/10/16 01:04:03 INFO executor.Executor: Finished task 3.0 in stage 7.0 (TID 17). 2210 bytes result sent to driver
15/10/16 01:04:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 7.0 (TID 17) in 516 ms on localhost (4/4)
15/10/16 01:04:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Stage 7 (collect at <console>:27) finished in 0.519 s
15/10/16 01:04:03 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:27, took 0.937249 s
(ztswDToyZGyL_dxo0CEQew,((200 W SenecaIthaca, NY 14850,List(Food, Ice Cream & Frozen Yogurt)),5.0))
(ztswDToyZGyL_dxo0CEQew,((200 W SenecaIthaca, NY 14850,List(Food, Ice Cream & Frozen Yogurt)),5.0))
(zh2Eja5h54vM7GUibHoi9A,((275 Westminster StDownCityProvidence, RI 02903,List(Art Supplies, Shopping, Arts & Crafts)),5.0))
(zh2Eja5h54vM7GUibHoi9A,((275 Westminster StDownCityProvidence, RI 02903,List(Art Supplies, Shopping, Arts & Crafts)),5.0))
(zkD9AtBT9ZkLFiV31gvEGw,((194 Waterman St3rd FlProvidence, RI 02906,List(Massage, Beauty and Spas)),5.0))
(zkD9AtBT9ZkLFiV31gvEGw,((194 Waterman St3rd FlProvidence, RI 02906,List(Massage, Beauty and Spas)),5.0))
(zrmO-d-Mw3kv9dWa7Trr9Q,((1101 Welch RdSte A1Palo Alto, CA 94304,List(Doctors, Health and Medical, Pediatricians)),5.0))
(zrmO-d-Mw3kv9dWa7Trr9Q,((1101 Welch RdSte A1Palo Alto, CA 94304,List(Doctors, Health and Medical, Pediatricians)),5.0))
(zx1lLUvlRUN5nQlSj3HRDw,((282 River StTroy, NY 12180,List(Shopping, Antiques)),5.0))
(zx1lLUvlRUN5nQlSj3HRDw,((282 River StTroy, NY 12180,List(Shopping, Antiques)),5.0))
(zqZjrcTf9Bc7r_VLGN4mrw,((1734 Elton Rd Ste 104Silver Spring, MD 20903,List(Doctors, Ear Nose & Throat, Health and Medical)),5.0))
(zqZjrcTf9Bc7r_VLGN4mrw,((1734 Elton Rd Ste 104Silver Spring, MD 20903,List(Doctors, Ear Nose & Throat, Health and Medical)),5.0))
(ziS_fZ7Z99fa4qNVr879vg,((141 The CommonsSte 3Ithaca, NY 14850,List(Day Spas, Massage, Beauty and Spas)),5.0))
(ziS_fZ7Z99fa4qNVr879vg,((141 The CommonsSte 3Ithaca, NY 14850,List(Day Spas, Massage, Beauty and Spas)),5.0))
(zh-MNFY4TyG7x2itUVlESQ,((University DistrictSeattle, WA 98145,List(Home Services, Home Inspectors)),5.0))
(zh-MNFY4TyG7x2itUVlESQ,((University DistrictSeattle, WA 98145,List(Home Services, Home Inspectors)),5.0))
(zweAzZDJ8SfwUID1UEGkbA,((701 Main StLafayette, IN 47901,List(Local Services, Printing Services)),5.0))
(zweAzZDJ8SfwUID1UEGkbA,((701 Main StLafayette, IN 47901,List(Local Services, Printing Services)),5.0))
(zvo21oKr656PQNVblYxYlg,((150 E Tenth StClaremont, CA 91711,List(Colleges & Universities, Education)),5.0))
(zvo21oKr656PQNVblYxYlg,((150 E Tenth StClaremont, CA 91711,List(Colleges & Universities, Education)),5.0))

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
{cs6360:~} clear

{cs6360:~} spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/10/16 01:09:38 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 01:09:38 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 01:09:38 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 01:09:38 INFO spark.HttpServer: Starting HTTP Server
15/10/16 01:09:38 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 01:09:38 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:42621
15/10/16 01:09:38 INFO util.Utils: Successfully started service 'HTTP class server' on port 42621.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/16 01:09:45 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/16 01:09:45 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/16 01:09:45 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/16 01:09:45 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/10/16 01:09:45 INFO Remoting: Starting remoting
15/10/16 01:09:46 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@cs6360.utdallas.edu:38934]
15/10/16 01:09:46 INFO util.Utils: Successfully started service 'sparkDriver' on port 38934.
15/10/16 01:09:46 INFO spark.SparkEnv: Registering MapOutputTracker
15/10/16 01:09:46 INFO spark.SparkEnv: Registering BlockManagerMaster
15/10/16 01:09:46 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20151016010946-d963
15/10/16 01:09:46 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/10/16 01:09:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/16 01:09:46 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-4273e199-660d-4c0c-a80a-08516faa296d
15/10/16 01:09:46 INFO spark.HttpServer: Starting HTTP Server
15/10/16 01:09:46 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 01:09:46 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:46637
15/10/16 01:09:46 INFO util.Utils: Successfully started service 'HTTP file server' on port 46637.
15/10/16 01:09:47 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 01:09:47 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 01:09:47 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@2c4a9e27: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 01:09:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/10/16 01:09:47 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 01:09:47 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 01:09:47 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@812886f: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/16 01:09:47 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/16 01:09:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
15/10/16 01:09:47 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/16 01:09:47 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4042
15/10/16 01:09:47 INFO util.Utils: Successfully started service 'SparkUI' on port 4042.
15/10/16 01:09:47 INFO ui.SparkUI: Started SparkUI at http://cs6360.utdallas.edu:4042
15/10/16 01:09:47 INFO executor.Executor: Using REPL class URI: http://10.176.92.90:42621
15/10/16 01:09:47 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@cs6360.utdallas.edu:38934/user/HeartbeatReceiver
15/10/16 01:09:47 INFO netty.NettyBlockTransferService: Server created on 40294
15/10/16 01:09:47 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/10/16 01:09:47 INFO storage.BlockManagerMasterActor: Registering block manager localhost:40294 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 40294)
15/10/16 01:09:47 INFO storage.BlockManagerMaster: Registered BlockManager
15/10/16 01:09:47 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.

scala> val businessFileMap = sc.textFile("/yelpdatafall/business/business.csv").map(l=> l.split("\\^")).map(l => (l(0),(l(1),l(2))))
15/10/16 01:10:21 INFO storage.MemoryStore: ensureFreeSpace(166036) called with curMem=0, maxMem=278302556
15/10/16 01:10:21 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 162.1 KB, free 265.3 MB)
15/10/16 01:10:21 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=166036, maxMem=278302556
15/10/16 01:10:21 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.2 MB)
15/10/16 01:10:21 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40294 (size: 22.7 KB, free: 265.4 MB)
15/10/16 01:10:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/10/16 01:10:21 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:12
businessFileMap: org.apache.spark.rdd.RDD[(String, (String, String))] = MappedRDD[3] at map at <console>:12

scala> var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
15/10/16 01:10:29 INFO storage.MemoryStore: ensureFreeSpace(166084) called with curMem=189325, maxMem=278302556
15/10/16 01:10:29 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 162.2 KB, free 265.1 MB)
15/10/16 01:10:29 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=355409, maxMem=278302556
15/10/16 01:10:29 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.0 MB)
15/10/16 01:10:29 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:40294 (size: 22.7 KB, free: 265.4 MB)
15/10/16 01:10:29 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/10/16 01:10:29 INFO spark.SparkContext: Created broadcast 1 from textFile at <console>:12
reviewFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/review/review.csv MappedRDD[5] at textFile at <console>:12

scala> var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
15/10/16 01:10:42 INFO mapred.FileInputFormat: Total input paths to process : 1
reviewFileMap: org.apache.spark.rdd.RDD[(String, Iterable[Double])] = ShuffledRDD[8] at groupByKey at <console>:14

scala> 

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     |      }     
<init>                           DRIVER_IDENTIFIER                SPARK_JOB_DESCRIPTION            SPARK_JOB_GROUP_ID               
SPARK_JOB_INTERRUPT_ON_CANCEL    SPARK_UNKNOWN_USER               boolToBoolWritable               booleanWritableConverter         
businessFileMap                  bytesToBytesWritable             bytesWritableConverter           classOf                          
clearActiveContext               clone                            doubleRDDToDoubleRDDFunctions    doubleToDoubleWritable           
doubleWritableConverter          eq                               equals                           finalize                         
floatToFloatWritable             floatWritableConverter           getClass                         hashCode                         
intToIntWritable                 intWritableConverter             isTraceEnabled                   jarOfClass                       
jarOfObject                      log                              logDebug                         logError                         
logInfo                          logName                          logTrace                         logWarning                       
longToLongWritable               longWritableConverter            markPartiallyConstructed         ne                               
notify                           notifyAll                        numericRDDToDoubleRDDFunctions   rddToAsyncRDDActions             
rddToOrderedRDDFunctions         rddToPairRDDFunctions            rddToSequenceFileRDDFunctions    reviewFile                       
reviewFileMap                    sc                               setActiveContext                 stringToText                     
stringWritableConverter          synchronized                     toString                         updatedConf                      
wait                             writableWritableConverter        
     |      }     
myAvgFunc: [T](a: Iterable[T])(implicit num: Numeric[T])Double

scala> val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
15/10/16 01:11:10 INFO spark.SparkContext: Starting job: sortByKey at <console>:18
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Registering RDD 7 (map at <console>:14)
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Got job 0 (sortByKey at <console>:18) with 2 output partitions (allowLocal=false)
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Final stage: Stage 1(sortByKey at <console>:18)
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 0)
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Missing parents: List(Stage 0)
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[7] at map at <console>:14), which has no missing parents
15/10/16 01:11:10 INFO storage.MemoryStore: ensureFreeSpace(3568) called with curMem=378698, maxMem=278302556
15/10/16 01:11:10 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.5 KB, free 265.0 MB)
15/10/16 01:11:10 INFO storage.MemoryStore: ensureFreeSpace(2523) called with curMem=382266, maxMem=278302556
15/10/16 01:11:10 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.0 MB)
15/10/16 01:11:10 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40294 (size: 2.5 KB, free: 265.4 MB)
15/10/16 01:11:10 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/10/16 01:11:10 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/10/16 01:11:10 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[7] at map at <console>:14)
15/10/16 01:11:10 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/10/16 01:11:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1300 bytes)
15/10/16 01:11:10 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1300 bytes)
15/10/16 01:11:10 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
15/10/16 01:11:10 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/16 01:11:10 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:0+12047591
15/10/16 01:11:10 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:12047591+12047592
15/10/16 01:11:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/10/16 01:11:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/10/16 01:11:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/10/16 01:11:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/10/16 01:11:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/10/16 01:11:12 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1896 bytes result sent to driver
15/10/16 01:11:12 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1896 bytes result sent to driver
15/10/16 01:11:12 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1642 ms on localhost (1/2)
15/10/16 01:11:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1672 ms on localhost (2/2)
15/10/16 01:11:12 INFO scheduler.DAGScheduler: Stage 0 (map at <console>:14) finished in 1.686 s
15/10/16 01:11:12 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:11:12 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:11:12 INFO scheduler.DAGScheduler: waiting: Set(Stage 1)
15/10/16 01:11:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/16 01:11:12 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:11:12 INFO scheduler.DAGScheduler: Missing parents for Stage 1: List()
15/10/16 01:11:12 INFO scheduler.DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[11] at sortByKey at <console>:18), which is now runnable
15/10/16 01:11:12 INFO storage.MemoryStore: ensureFreeSpace(6448) called with curMem=384789, maxMem=278302556
15/10/16 01:11:12 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.3 KB, free 265.0 MB)
15/10/16 01:11:12 INFO storage.MemoryStore: ensureFreeSpace(4209) called with curMem=391237, maxMem=278302556
15/10/16 01:11:12 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.0 MB)
15/10/16 01:11:12 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:40294 (size: 4.1 KB, free: 265.4 MB)
15/10/16 01:11:12 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/10/16 01:11:12 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/10/16 01:11:12 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[11] at sortByKey at <console>:18)
15/10/16 01:11:12 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/10/16 01:11:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:11:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:11:12 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
15/10/16 01:11:12 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
15/10/16 01:11:12 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:12 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/10/16 01:11:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/10/16 01:11:13 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2530 bytes result sent to driver
15/10/16 01:11:13 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2530 bytes result sent to driver
15/10/16 01:11:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1404 ms on localhost (1/2)
15/10/16 01:11:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1411 ms on localhost (2/2)
15/10/16 01:11:13 INFO scheduler.DAGScheduler: Stage 1 (sortByKey at <console>:18) finished in 1.412 s
15/10/16 01:11:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/10/16 01:11:13 INFO scheduler.DAGScheduler: Job 0 finished: sortByKey at <console>:18, took 3.231570 s
ab: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[12] at sortByKey at <console>:18

scala> val rate = ab.map(l => (l._2, l._1)). top(10).map(l=>(l._2,l._1))
15/10/16 01:11:18 INFO spark.SparkContext: Starting job: top at <console>:20
15/10/16 01:11:18 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Registering RDD 9 (map at <console>:18)
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Got job 1 (top at <console>:20) with 2 output partitions (allowLocal=false)
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Final stage: Stage 4(top at <console>:20)
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 3)
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Missing parents: List(Stage 3)
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Submitting Stage 3 (MappedRDD[9] at map at <console>:18), which has no missing parents
15/10/16 01:11:18 INFO storage.MemoryStore: ensureFreeSpace(6488) called with curMem=395446, maxMem=278302556
15/10/16 01:11:18 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.3 KB, free 265.0 MB)
15/10/16 01:11:18 INFO storage.MemoryStore: ensureFreeSpace(4245) called with curMem=401934, maxMem=278302556
15/10/16 01:11:18 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.0 MB)
15/10/16 01:11:18 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:40294 (size: 4.1 KB, free: 265.4 MB)
15/10/16 01:11:18 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/10/16 01:11:18 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:838
15/10/16 01:11:18 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 3 (MappedRDD[9] at map at <console>:18)
15/10/16 01:11:18 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
15/10/16 01:11:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:11:18 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:11:18 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 4)
15/10/16 01:11:18 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 5)
15/10/16 01:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:11:19 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1001 bytes result sent to driver
15/10/16 01:11:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 1454 ms on localhost (1/2)
15/10/16 01:11:19 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1001 bytes result sent to driver
15/10/16 01:11:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 1463 ms on localhost (2/2)
15/10/16 01:11:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/10/16 01:11:19 INFO scheduler.DAGScheduler: Stage 3 (map at <console>:18) finished in 1.465 s
15/10/16 01:11:19 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:11:19 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:11:19 INFO scheduler.DAGScheduler: waiting: Set(Stage 4)
15/10/16 01:11:19 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:11:19 INFO scheduler.DAGScheduler: Missing parents for Stage 4: List()
15/10/16 01:11:19 INFO scheduler.DAGScheduler: Submitting Stage 4 (MapPartitionsRDD[14] at top at <console>:20), which is now runnable
15/10/16 01:11:19 INFO storage.MemoryStore: ensureFreeSpace(3376) called with curMem=406179, maxMem=278302556
15/10/16 01:11:19 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 265.0 MB)
15/10/16 01:11:19 INFO storage.MemoryStore: ensureFreeSpace(2395) called with curMem=409555, maxMem=278302556
15/10/16 01:11:19 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 265.0 MB)
15/10/16 01:11:19 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:40294 (size: 2.3 KB, free: 265.4 MB)
15/10/16 01:11:19 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/10/16 01:11:19 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:838
15/10/16 01:11:19 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 4 (MapPartitionsRDD[14] at top at <console>:20)
15/10/16 01:11:19 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
15/10/16 01:11:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:11:19 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:11:19 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 6)
15/10/16 01:11:19 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 7)
15/10/16 01:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:11:20 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 6). 1833 bytes result sent to driver
15/10/16 01:11:20 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 7). 1833 bytes result sent to driver
15/10/16 01:11:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 257 ms on localhost (1/2)
15/10/16 01:11:20 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 257 ms on localhost (2/2)
15/10/16 01:11:20 INFO scheduler.DAGScheduler: Stage 4 (top at <console>:20) finished in 0.261 s
15/10/16 01:11:20 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/10/16 01:11:20 INFO scheduler.DAGScheduler: Job 1 finished: top at <console>:20, took 1.784449 s
rate: Array[(String, Double)] = Array((zx1lLUvlRUN5nQlSj3HRDw,5.0), (zweAzZDJ8SfwUID1UEGkbA,5.0), (zvo21oKr656PQNVblYxYlg,5.0), (ztswDToyZGyL_dxo0CEQew,5.0), (zrmO-d-Mw3kv9dWa7Trr9Q,5.0), (zqZjrcTf9Bc7r_VLGN4mrw,5.0), (zkD9AtBT9ZkLFiV31gvEGw,5.0), (ziS_fZ7Z99fa4qNVr879vg,5.0), (zh2Eja5h54vM7GUibHoi9A,5.0), (zh-MNFY4TyG7x2itUVlESQ,5.0))

scala> 

scala> var finalOP1broadcast = sc.broadcast(rate)
15/10/16 01:11:25 INFO storage.MemoryStore: ensureFreeSpace(1440) called with curMem=411950, maxMem=278302556
15/10/16 01:11:25 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1440.0 B, free 265.0 MB)
15/10/16 01:11:25 INFO storage.MemoryStore: ensureFreeSpace(453) called with curMem=413390, maxMem=278302556
15/10/16 01:11:25 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 453.0 B, free 265.0 MB)
15/10/16 01:11:25 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:40294 (size: 453.0 B, free: 265.4 MB)
15/10/16 01:11:25 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/10/16 01:11:25 INFO spark.SparkContext: Created broadcast 6 from broadcast at <console>:22
finalOP1broadcast: org.apache.spark.broadcast.Broadcast[Array[(String, Double)]] = Broadcast(6)

scala> businessFileMap.map( k => if(finalOP1broadcast.value.contains(k._1)) k else None).filter(k=>k!=(None).collect().foreach(println)
     | 
     | 
You typed two blank lines.  Starting a new command.

scala> businessFileMap.map{k => if(finalOP1broadcast.value.contains(k._1)) k else None}.filter(k=>k!=(None)).collect().foreach(println)
15/10/16 01:12:29 INFO mapred.FileInputFormat: Total input paths to process : 1
15/10/16 01:12:29 INFO spark.SparkContext: Starting job: collect at <console>:27
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Got job 2 (collect at <console>:27) with 2 output partitions (allowLocal=false)
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Final stage: Stage 5(collect at <console>:27)
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Missing parents: List()
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Submitting Stage 5 (FilteredRDD[16] at filter at <console>:27), which has no missing parents
15/10/16 01:12:29 INFO storage.MemoryStore: ensureFreeSpace(10432) called with curMem=413843, maxMem=278302556
15/10/16 01:12:29 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.2 KB, free 265.0 MB)
15/10/16 01:12:29 INFO storage.MemoryStore: ensureFreeSpace(6512) called with curMem=424275, maxMem=278302556
15/10/16 01:12:29 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 265.0 MB)
15/10/16 01:12:29 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:40294 (size: 6.4 KB, free: 265.3 MB)
15/10/16 01:12:29 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/10/16 01:12:29 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:838
15/10/16 01:12:29 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 5 (FilteredRDD[16] at filter at <console>:27)
15/10/16 01:12:29 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
15/10/16 01:12:29 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 8, localhost, ANY, 1315 bytes)
15/10/16 01:12:29 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 9, localhost, ANY, 1315 bytes)
15/10/16 01:12:30 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 8)
15/10/16 01:12:30 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 9)
15/10/16 01:12:30 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:1486763+1486763
15/10/16 01:12:30 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:0+1486763
15/10/16 01:12:30 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 9). 1719 bytes result sent to driver
15/10/16 01:12:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 9) in 237 ms on localhost (1/2)
15/10/16 01:12:30 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 8). 1719 bytes result sent to driver
15/10/16 01:12:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 8) in 243 ms on localhost (2/2)
15/10/16 01:12:30 INFO scheduler.DAGScheduler: Stage 5 (collect at <console>:27) finished in 0.242 s
15/10/16 01:12:30 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/10/16 01:12:30 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:27, took 0.265627 s

scala> businessFileMap.map{k => if(finalOP1broadcast.value.contains(k._1)) k else None}.filter(k=>k!=(None)).collect().foreach(println)
15/10/16 01:12:52 INFO spark.SparkContext: Starting job: collect at <console>:27
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Got job 3 (collect at <console>:27) with 2 output partitions (allowLocal=false)
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Final stage: Stage 6(collect at <console>:27)
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Missing parents: List()
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Submitting Stage 6 (FilteredRDD[18] at filter at <console>:27), which has no missing parents
15/10/16 01:12:52 INFO storage.MemoryStore: ensureFreeSpace(10432) called with curMem=430787, maxMem=278302556
15/10/16 01:12:52 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.2 KB, free 265.0 MB)
15/10/16 01:12:52 INFO storage.MemoryStore: ensureFreeSpace(6517) called with curMem=441219, maxMem=278302556
15/10/16 01:12:52 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KB, free 265.0 MB)
15/10/16 01:12:52 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:40294 (size: 6.4 KB, free: 265.3 MB)
15/10/16 01:12:52 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/10/16 01:12:52 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:838
15/10/16 01:12:52 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (FilteredRDD[18] at filter at <console>:27)
15/10/16 01:12:52 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
15/10/16 01:12:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10, localhost, ANY, 1315 bytes)
15/10/16 01:12:52 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 11, localhost, ANY, 1315 bytes)
15/10/16 01:12:52 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 10)
15/10/16 01:12:52 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 11)
15/10/16 01:12:52 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:1486763+1486763
15/10/16 01:12:52 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:0+1486763
15/10/16 01:12:53 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 10). 1719 bytes result sent to driver
15/10/16 01:12:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 185 ms on localhost (1/2)
15/10/16 01:12:53 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 11). 1719 bytes result sent to driver
15/10/16 01:12:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 11) in 190 ms on localhost (2/2)
15/10/16 01:12:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/10/16 01:12:53 INFO scheduler.DAGScheduler: Stage 6 (collect at <console>:27) finished in 0.196 s
15/10/16 01:12:53 INFO scheduler.DAGScheduler: Job 3 finished: collect at <console>:27, took 0.211460 s

scala> val businessFileMap = sc.textFile("/yelpdatafall/business/business.csv").map(l=> l.split("\\^")).map(l => (l(0),(l(1),l(2))))
15/10/16 01:13:36 INFO storage.MemoryStore: ensureFreeSpace(81443) called with curMem=447736, maxMem=278302556
15/10/16 01:13:36 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 79.5 KB, free 264.9 MB)
15/10/16 01:13:37 INFO storage.MemoryStore: ensureFreeSpace(32041) called with curMem=529179, maxMem=278302556
15/10/16 01:13:37 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.3 KB, free 264.9 MB)
15/10/16 01:13:37 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:40294 (size: 31.3 KB, free: 265.3 MB)
15/10/16 01:13:37 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/10/16 01:13:37 INFO spark.SparkContext: Created broadcast 9 from textFile at <console>:12
businessFileMap: org.apache.spark.rdd.RDD[(String, (String, String))] = MappedRDD[22] at map at <console>:12

scala> 

scala> var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
15/10/16 01:13:37 INFO storage.MemoryStore: ensureFreeSpace(217562) called with curMem=561220, maxMem=278302556
15/10/16 01:13:37 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 212.5 KB, free 264.7 MB)
15/10/16 01:13:37 INFO storage.MemoryStore: ensureFreeSpace(32041) called with curMem=778782, maxMem=278302556
15/10/16 01:13:37 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 31.3 KB, free 264.6 MB)
15/10/16 01:13:37 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:40294 (size: 31.3 KB, free: 265.3 MB)
15/10/16 01:13:37 INFO storage.BlockManagerMaster: Updated info of block broadcast_10_piece0
15/10/16 01:13:37 INFO spark.SparkContext: Created broadcast 10 from textFile at <console>:12
reviewFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/review/review.csv MappedRDD[24] at textFile at <console>:12

scala> var reviewFileMap = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
15/10/16 01:13:37 INFO mapred.FileInputFormat: Total input paths to process : 1
reviewFileMap: org.apache.spark.rdd.RDD[(String, Iterable[Double])] = ShuffledRDD[27] at groupByKey at <console>:14

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |      num.toDouble( a.sum ) / a.size
     |      }     
<init>                           DRIVER_IDENTIFIER                SPARK_JOB_DESCRIPTION            SPARK_JOB_GROUP_ID               
SPARK_JOB_INTERRUPT_ON_CANCEL    SPARK_UNKNOWN_USER               ab                               boolToBoolWritable               
booleanWritableConverter         businessFileMap                  bytesToBytesWritable             bytesWritableConverter           
classOf                          clearActiveContext               clone                            doubleRDDToDoubleRDDFunctions    
doubleToDoubleWritable           doubleWritableConverter          eq                               equals                           
finalOP1broadcast                finalize                         floatToFloatWritable             floatWritableConverter           
getClass                         hashCode                         intToIntWritable                 intWritableConverter             
isTraceEnabled                   jarOfClass                       jarOfObject                      log                              
logDebug                         logError                         logInfo                          logName                          
logTrace                         logWarning                       longToLongWritable               longWritableConverter            
markPartiallyConstructed         myAvgFunc                        ne                               notify                           
notifyAll                        numericRDDToDoubleRDDFunctions   rate                             rddToAsyncRDDActions             
rddToOrderedRDDFunctions         rddToPairRDDFunctions            rddToSequenceFileRDDFunctions    res0                             
res1                             reviewFile                       reviewFileMap                    sc                               
setActiveContext                 stringToText                     stringWritableConverter          synchronized                     
toString                         updatedConf                      wait                             writableWritableConverter        
     |      }     
myAvgFunc: [T](a: Iterable[T])(implicit num: Numeric[T])Double

scala> val ab = reviewFileMap.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
15/10/16 01:13:38 INFO spark.SparkContext: Starting job: sortByKey at <console>:18
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Registering RDD 26 (map at <console>:14)
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Got job 4 (sortByKey at <console>:18) with 2 output partitions (allowLocal=false)
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Final stage: Stage 8(sortByKey at <console>:18)
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 7)
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Missing parents: List(Stage 7)
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Submitting Stage 7 (MappedRDD[26] at map at <console>:14), which has no missing parents
15/10/16 01:13:38 INFO storage.MemoryStore: ensureFreeSpace(3568) called with curMem=810823, maxMem=278302556
15/10/16 01:13:38 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 3.5 KB, free 264.6 MB)
15/10/16 01:13:38 INFO storage.MemoryStore: ensureFreeSpace(2523) called with curMem=814391, maxMem=278302556
15/10/16 01:13:38 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.5 KB, free 264.6 MB)
15/10/16 01:13:38 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:40294 (size: 2.5 KB, free: 265.3 MB)
15/10/16 01:13:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_11_piece0
15/10/16 01:13:38 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/10/16 01:13:38 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 7 (MappedRDD[26] at map at <console>:14)
15/10/16 01:13:38 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
15/10/16 01:13:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 12, localhost, ANY, 1300 bytes)
15/10/16 01:13:38 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 13, localhost, ANY, 1300 bytes)
15/10/16 01:13:38 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 12)
15/10/16 01:13:38 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:0+12047591
15/10/16 01:13:38 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 13)
15/10/16 01:13:38 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:12047591+12047592
15/10/16 01:13:39 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 13). 2059 bytes result sent to driver
15/10/16 01:13:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 13) in 1163 ms on localhost (1/2)
15/10/16 01:13:40 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 12). 2059 bytes result sent to driver
15/10/16 01:13:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 12) in 1219 ms on localhost (2/2)
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Stage 7 (map at <console>:14) finished in 1.219 s
15/10/16 01:13:40 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:13:40 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:13:40 INFO scheduler.DAGScheduler: waiting: Set(Stage 8)
15/10/16 01:13:40 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:13:40 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Missing parents for Stage 8: List()
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[30] at sortByKey at <console>:18), which is now runnable
15/10/16 01:13:40 INFO storage.MemoryStore: ensureFreeSpace(6464) called with curMem=816914, maxMem=278302556
15/10/16 01:13:40 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 6.3 KB, free 264.6 MB)
15/10/16 01:13:40 INFO storage.MemoryStore: ensureFreeSpace(4230) called with curMem=823378, maxMem=278302556
15/10/16 01:13:40 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.6 MB)
15/10/16 01:13:40 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:40294 (size: 4.1 KB, free: 265.3 MB)
15/10/16 01:13:40 INFO storage.BlockManagerMaster: Updated info of block broadcast_12_piece0
15/10/16 01:13:40 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 8 (MapPartitionsRDD[30] at sortByKey at <console>:18)
15/10/16 01:13:40 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
15/10/16 01:13:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 14, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:13:40 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 15, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:13:40 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 15)
15/10/16 01:13:40 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 14)
15/10/16 01:13:40 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:40 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:13:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:13:40 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 14). 2530 bytes result sent to driver
15/10/16 01:13:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 14) in 770 ms on localhost (1/2)
15/10/16 01:13:40 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 15). 2530 bytes result sent to driver
15/10/16 01:13:40 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 15) in 789 ms on localhost (2/2)
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Stage 8 (sortByKey at <console>:18) finished in 0.791 s
15/10/16 01:13:40 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/10/16 01:13:40 INFO scheduler.DAGScheduler: Job 4 finished: sortByKey at <console>:18, took 2.042572 s
ab: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[31] at sortByKey at <console>:18

scala> val rate = ab.map(l => (l._2, l._1)). top(10).map(l=>(l._2,l._1))
15/10/16 01:13:41 INFO spark.SparkContext: Starting job: top at <console>:20
15/10/16 01:13:41 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 154 bytes
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Registering RDD 28 (map at <console>:18)
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Got job 5 (top at <console>:20) with 2 output partitions (allowLocal=false)
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Final stage: Stage 11(top at <console>:20)
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 10)
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Missing parents: List(Stage 10)
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Submitting Stage 10 (MappedRDD[28] at map at <console>:18), which has no missing parents
15/10/16 01:13:41 INFO storage.MemoryStore: ensureFreeSpace(6512) called with curMem=827608, maxMem=278302556
15/10/16 01:13:41 INFO storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 6.4 KB, free 264.6 MB)
15/10/16 01:13:41 INFO storage.MemoryStore: ensureFreeSpace(4262) called with curMem=834120, maxMem=278302556
15/10/16 01:13:41 INFO storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.2 KB, free 264.6 MB)
15/10/16 01:13:41 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:40294 (size: 4.2 KB, free: 265.3 MB)
15/10/16 01:13:41 INFO storage.BlockManagerMaster: Updated info of block broadcast_13_piece0
15/10/16 01:13:41 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/10/16 01:13:41 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 10 (MappedRDD[28] at map at <console>:18)
15/10/16 01:13:41 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
15/10/16 01:13:41 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 16, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:13:41 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 17, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/16 01:13:41 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 17)
15/10/16 01:13:41 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 16)
15/10/16 01:13:41 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:13:41 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/16 01:13:42 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 16). 1001 bytes result sent to driver
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 16) in 728 ms on localhost (1/2)
15/10/16 01:13:42 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 17). 1001 bytes result sent to driver
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 17) in 742 ms on localhost (2/2)
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Stage 10 (map at <console>:18) finished in 0.743 s
15/10/16 01:13:42 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/16 01:13:42 INFO scheduler.DAGScheduler: running: Set()
15/10/16 01:13:42 INFO scheduler.DAGScheduler: waiting: Set(Stage 11)
15/10/16 01:13:42 INFO scheduler.DAGScheduler: failed: Set()
15/10/16 01:13:42 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Missing parents for Stage 11: List()
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[33] at top at <console>:20), which is now runnable
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(3376) called with curMem=838382, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 3.3 KB, free 264.6 MB)
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(2396) called with curMem=841758, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.3 KB, free 264.6 MB)
15/10/16 01:13:42 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:40294 (size: 2.3 KB, free: 265.3 MB)
15/10/16 01:13:42 INFO storage.BlockManagerMaster: Updated info of block broadcast_14_piece0
15/10/16 01:13:42 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:838
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 11 (MapPartitionsRDD[33] at top at <console>:20)
15/10/16 01:13:42 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 18, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 19, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/16 01:13:42 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 19)
15/10/16 01:13:42 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 18)
15/10/16 01:13:42 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:42 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/16 01:13:42 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:13:42 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/16 01:13:42 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 19). 1833 bytes result sent to driver
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 19) in 65 ms on localhost (1/2)
15/10/16 01:13:42 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 18). 1833 bytes result sent to driver
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 18) in 69 ms on localhost (2/2)
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Stage 11 (top at <console>:20) finished in 0.070 s
15/10/16 01:13:42 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Job 5 finished: top at <console>:20, took 0.841067 s
rate: Array[(String, Double)] = Array((zx1lLUvlRUN5nQlSj3HRDw,5.0), (zweAzZDJ8SfwUID1UEGkbA,5.0), (zvo21oKr656PQNVblYxYlg,5.0), (ztswDToyZGyL_dxo0CEQew,5.0), (zrmO-d-Mw3kv9dWa7Trr9Q,5.0), (zqZjrcTf9Bc7r_VLGN4mrw,5.0), (zkD9AtBT9ZkLFiV31gvEGw,5.0), (ziS_fZ7Z99fa4qNVr879vg,5.0), (zh2Eja5h54vM7GUibHoi9A,5.0), (zh-MNFY4TyG7x2itUVlESQ,5.0))

scala> var finalOP1broadcast = sc.broadcast(rate)
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(1440) called with curMem=844154, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 1440.0 B, free 264.6 MB)
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(453) called with curMem=845594, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 453.0 B, free 264.6 MB)
15/10/16 01:13:42 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:40294 (size: 453.0 B, free: 265.3 MB)
15/10/16 01:13:42 INFO storage.BlockManagerMaster: Updated info of block broadcast_15_piece0
15/10/16 01:13:42 INFO spark.SparkContext: Created broadcast 15 from broadcast at <console>:22
finalOP1broadcast: org.apache.spark.broadcast.Broadcast[Array[(String, Double)]] = Broadcast(15)

scala> 

scala> businessFileMap.map{k => if(finalOP1broadcast.value.contains(k._1)) k else None}.filter(k=>k!=(None)).collect().foreach(println)
15/10/16 01:13:42 INFO mapred.FileInputFormat: Total input paths to process : 1
15/10/16 01:13:42 INFO spark.SparkContext: Starting job: collect at <console>:27
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Got job 6 (collect at <console>:27) with 2 output partitions (allowLocal=false)
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Final stage: Stage 12(collect at <console>:27)
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Missing parents: List()
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Submitting Stage 12 (FilteredRDD[35] at filter at <console>:27), which has no missing parents
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(10464) called with curMem=846047, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 10.2 KB, free 264.6 MB)
15/10/16 01:13:42 INFO storage.MemoryStore: ensureFreeSpace(6538) called with curMem=856511, maxMem=278302556
15/10/16 01:13:42 INFO storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.4 KB, free 264.6 MB)
15/10/16 01:13:42 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:40294 (size: 6.4 KB, free: 265.3 MB)
15/10/16 01:13:42 INFO storage.BlockManagerMaster: Updated info of block broadcast_16_piece0
15/10/16 01:13:42 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/10/16 01:13:42 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 12 (FilteredRDD[35] at filter at <console>:27)
15/10/16 01:13:42 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 20, localhost, ANY, 1315 bytes)
15/10/16 01:13:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 21, localhost, ANY, 1315 bytes)
15/10/16 01:13:42 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 21)
15/10/16 01:13:42 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 20)
15/10/16 01:13:42 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:0+1486763
15/10/16 01:13:42 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:1486763+1486763
15/10/16 01:13:43 INFO storage.BlockManager: Removing broadcast 14
15/10/16 01:13:43 INFO storage.BlockManager: Removing block broadcast_14_piece0
15/10/16 01:13:43 INFO storage.MemoryStore: Block broadcast_14_piece0 of size 2396 dropped from memory (free 277441903)
15/10/16 01:13:43 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on localhost:40294 in memory (size: 2.3 KB, free: 265.3 MB)
15/10/16 01:13:43 INFO storage.BlockManagerMaster: Updated info of block broadcast_14_piece0
15/10/16 01:13:43 INFO storage.BlockManager: Removing block broadcast_14
15/10/16 01:13:43 INFO storage.MemoryStore: Block broadcast_14 of size 3376 dropped from memory (free 277445279)
15/10/16 01:13:43 INFO spark.ContextCleaner: Cleaned broadcast 14
15/10/16 01:13:43 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 21). 1719 bytes result sent to driver
15/10/16 01:13:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 21) in 228 ms on localhost (1/2)
15/10/16 01:13:43 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 20). 1719 bytes result sent to driver
15/10/16 01:13:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 20) in 234 ms on localhost (2/2)
15/10/16 01:13:43 INFO scheduler.DAGScheduler: Stage 12 (collect at <console>:27) finished in 0.234 s
15/10/16 01:13:43 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/10/16 01:13:43 INFO scheduler.DAGScheduler: Job 6 finished: collect at <console>:27, took 0.246397 s

scala> 

scala> businessFileMap.map{i => if(finalOP1broadcast.value.contains(i._1)) i else None}.filter(i=>i!=(None)).collect().foreach(println)
15/10/16 01:15:00 INFO spark.SparkContext: Starting job: collect at <console>:27
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Got job 7 (collect at <console>:27) with 2 output partitions (allowLocal=false)
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Final stage: Stage 13(collect at <console>:27)
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Missing parents: List()
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Submitting Stage 13 (FilteredRDD[37] at filter at <console>:27), which has no missing parents
15/10/16 01:15:00 INFO storage.MemoryStore: ensureFreeSpace(10464) called with curMem=857277, maxMem=278302556
15/10/16 01:15:00 INFO storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 10.2 KB, free 264.6 MB)
15/10/16 01:15:00 INFO storage.MemoryStore: ensureFreeSpace(6537) called with curMem=867741, maxMem=278302556
15/10/16 01:15:00 INFO storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.4 KB, free 264.6 MB)
15/10/16 01:15:00 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:40294 (size: 6.4 KB, free: 265.3 MB)
15/10/16 01:15:00 INFO storage.BlockManagerMaster: Updated info of block broadcast_17_piece0
15/10/16 01:15:00 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 13 (FilteredRDD[37] at filter at <console>:27)
15/10/16 01:15:00 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
15/10/16 01:15:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 22, localhost, ANY, 1315 bytes)
15/10/16 01:15:00 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 23, localhost, ANY, 1315 bytes)
15/10/16 01:15:00 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 22)
15/10/16 01:15:00 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 23)
15/10/16 01:15:00 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:0+1486763
15/10/16 01:15:00 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/business/business.csv:1486763+1486763
15/10/16 01:15:00 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 22). 1719 bytes result sent to driver
15/10/16 01:15:00 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 22) in 241 ms on localhost (1/2)
15/10/16 01:15:00 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 23). 1719 bytes result sent to driver
15/10/16 01:15:00 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 23) in 269 ms on localhost (2/2)
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Stage 13 (collect at <console>:27) finished in 0.270 s
15/10/16 01:15:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/10/16 01:15:00 INFO scheduler.DAGScheduler: Job 7 finished: collect at <console>:27, took 0.289858 s

scala> 

scala> 
