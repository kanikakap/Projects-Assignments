Last login: Thu Oct 15 23:12:07 on ttys000
kanikas-MacBook-Pro:~ kanikakapoor$ ssh kxk140230@cs6360.utdallas.edu

                 Department of Computer Science
                 University of Texas at Dallas

  Pursuant to Texas Administrative Code 202:

  (1) Unauthorized use is prohibited;

  (2) Usage may be subject to security testing and monitoring;

  (3) Misuse is subject to criminal prosecution; and

  (4) No expectation of privacy except as otherwise provided by
      applicable privacy laws.


kxk140230@cs6360.utdallas.edu's password: 
Last login: Thu Oct 15 23:12:30 2015 from 10.21.74.218
Sourcing /usr/local/etc/skel/global/profile
-bash: rt: command not found
{cs6360:~} spark-shell --executor-cores 6

Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/10/15 23:36:24 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/15 23:36:24 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/15 23:36:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/15 23:36:24 INFO spark.HttpServer: Starting HTTP Server
15/10/15 23:36:24 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/15 23:36:25 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:48119
15/10/15 23:36:25 INFO util.Utils: Successfully started service 'HTTP class server' on port 48119.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/15 23:36:32 INFO spark.SecurityManager: Changing view acls to: kxk140230
15/10/15 23:36:32 INFO spark.SecurityManager: Changing modify acls to: kxk140230
15/10/15 23:36:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kxk140230); users with modify permissions: Set(kxk140230)
15/10/15 23:36:33 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/10/15 23:36:33 INFO Remoting: Starting remoting
15/10/15 23:36:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@cs6360.utdallas.edu:60837]
15/10/15 23:36:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 60837.
15/10/15 23:36:34 INFO spark.SparkEnv: Registering MapOutputTracker
15/10/15 23:36:34 INFO spark.SparkEnv: Registering BlockManagerMaster
15/10/15 23:36:34 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20151015233634-ae8b
15/10/15 23:36:34 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/10/15 23:36:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/15 23:36:35 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-437b18b5-5414-4561-9887-2a251d307056
15/10/15 23:36:35 INFO spark.HttpServer: Starting HTTP Server
15/10/15 23:36:35 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/15 23:36:35 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:48891
15/10/15 23:36:35 INFO util.Utils: Successfully started service 'HTTP file server' on port 48891.
15/10/15 23:36:35 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/15 23:36:35 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/15 23:36:35 WARN component.AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@184c4ddb: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $line3.$read$$iwC$$iwC.<init>(<console>:9)
	at $line3.$read$$iwC.<init>(<console>:18)
	at $line3.$read.<init>(<console>:20)
	at $line3.$read$.<init>(<console>:24)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.<init>(<console>:7)
	at $line3.$eval$.<clinit>(<console>)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:270)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:147)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:60)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:962)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
15/10/15 23:36:35 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/10/15 23:36:35 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/10/15 23:36:35 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/10/15 23:36:35 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4041
15/10/15 23:36:35 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.
15/10/15 23:36:35 INFO ui.SparkUI: Started SparkUI at http://cs6360.utdallas.edu:4041
15/10/15 23:36:35 INFO executor.Executor: Using REPL class URI: http://10.176.92.90:48119
15/10/15 23:36:35 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@cs6360.utdallas.edu:60837/user/HeartbeatReceiver
15/10/15 23:36:35 INFO netty.NettyBlockTransferService: Server created on 58634
15/10/15 23:36:35 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/10/15 23:36:35 INFO storage.BlockManagerMasterActor: Registering block manager localhost:58634 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 58634)
15/10/15 23:36:35 INFO storage.BlockManagerMaster: Registered BlockManager
15/10/15 23:36:36 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.

scala> 

scala> var reviewFile = sc.textFile("/yelpdatafall/review/review.csv")
15/10/15 23:36:46 INFO storage.MemoryStore: ensureFreeSpace(166036) called with curMem=0, maxMem=278302556
15/10/15 23:36:46 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 162.1 KB, free 265.3 MB)
15/10/15 23:36:46 INFO storage.MemoryStore: ensureFreeSpace(23289) called with curMem=166036, maxMem=278302556
15/10/15 23:36:46 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.7 KB, free 265.2 MB)
15/10/15 23:36:46 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:58634 (size: 22.7 KB, free: 265.4 MB)
15/10/15 23:36:46 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/10/15 23:36:46 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:12
reviewFile: org.apache.spark.rdd.RDD[String] = /yelpdatafall/review/review.csv MappedRDD[1] at textFile at <console>:12

scala> def myAvgFunc[T]( a: Iterable[T] )( implicit num: Numeric[T] ) = {
     |    num.toDouble( a.sum ) / a.size
     | }
myAvgFunc: [T](a: Iterable[T])(implicit num: Numeric[T])Double

scala> var ab  = reviewFile.map(l => l.split("\\^")).map(l => (l(2),l(3).toDouble)).groupByKey()
15/10/15 23:37:05 INFO mapred.FileInputFormat: Total input paths to process : 1
ab: org.apache.spark.rdd.RDD[(String, Iterable[Double])] = ShuffledRDD[4] at groupByKey at <console>:14

scala> val kx = ab.map(l => (l._1, myAvgFunc (l._2))).sortByKey()
15/10/15 23:37:14 INFO spark.SparkContext: Starting job: sortByKey at <console>:18
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Registering RDD 3 (map at <console>:14)
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Got job 0 (sortByKey at <console>:18) with 2 output partitions (allowLocal=false)
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Final stage: Stage 1(sortByKey at <console>:18)
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 0)
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Missing parents: List(Stage 0)
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[3] at map at <console>:14), which has no missing parents
15/10/15 23:37:14 INFO storage.MemoryStore: ensureFreeSpace(3568) called with curMem=189325, maxMem=278302556
15/10/15 23:37:14 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 265.2 MB)
15/10/15 23:37:14 INFO storage.MemoryStore: ensureFreeSpace(2521) called with curMem=192893, maxMem=278302556
15/10/15 23:37:14 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.2 MB)
15/10/15 23:37:14 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:58634 (size: 2.5 KB, free: 265.4 MB)
15/10/15 23:37:14 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/10/15 23:37:14 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/10/15 23:37:14 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[3] at map at <console>:14)
15/10/15 23:37:14 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/10/15 23:37:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1300 bytes)
15/10/15 23:37:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1300 bytes)
15/10/15 23:37:14 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/15 23:37:14 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
15/10/15 23:37:14 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:12047591+12047592
15/10/15 23:37:14 INFO rdd.HadoopRDD: Input split: hdfs://cshadoop1/yelpdatafall/review/review.csv:0+12047591
15/10/15 23:37:14 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/10/15 23:37:14 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/10/15 23:37:14 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/10/15 23:37:14 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/10/15 23:37:14 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/10/15 23:37:16 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1896 bytes result sent to driver
15/10/15 23:37:16 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1844 ms on localhost (1/2)
15/10/15 23:37:16 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1896 bytes result sent to driver
15/10/15 23:37:16 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2052 ms on localhost (2/2)
15/10/15 23:37:16 INFO scheduler.DAGScheduler: Stage 0 (map at <console>:14) finished in 2.092 s
15/10/15 23:37:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/15 23:37:16 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/15 23:37:16 INFO scheduler.DAGScheduler: running: Set()
15/10/15 23:37:16 INFO scheduler.DAGScheduler: waiting: Set(Stage 1)
15/10/15 23:37:16 INFO scheduler.DAGScheduler: failed: Set()
15/10/15 23:37:16 INFO scheduler.DAGScheduler: Missing parents for Stage 1: List()
15/10/15 23:37:16 INFO scheduler.DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[7] at sortByKey at <console>:18), which is now runnable
15/10/15 23:37:16 INFO storage.MemoryStore: ensureFreeSpace(6424) called with curMem=195414, maxMem=278302556
15/10/15 23:37:16 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.3 KB, free 265.2 MB)
15/10/15 23:37:16 INFO storage.MemoryStore: ensureFreeSpace(4172) called with curMem=201838, maxMem=278302556
15/10/15 23:37:16 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.2 MB)
15/10/15 23:37:16 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:58634 (size: 4.1 KB, free: 265.4 MB)
15/10/15 23:37:16 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/10/15 23:37:16 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/10/15 23:37:16 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[7] at sortByKey at <console>:18)
15/10/15 23:37:16 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/10/15 23:37:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/15 23:37:16 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/15 23:37:16 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
15/10/15 23:37:16 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
15/10/15 23:37:16 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:16 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:16 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/10/15 23:37:16 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/10/15 23:37:18 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2530 bytes result sent to driver
15/10/15 23:37:18 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2530 bytes result sent to driver
15/10/15 23:37:18 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1681 ms on localhost (1/2)
15/10/15 23:37:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1687 ms on localhost (2/2)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Stage 1 (sortByKey at <console>:18) finished in 1.688 s
15/10/15 23:37:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Job 0 finished: sortByKey at <console>:18, took 3.932971 s
kx: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[8] at sortByKey at <console>:18

scala> val finalOP = kx.map(l => (l._2, l._1)). top(10).foreach(println)
15/10/15 23:37:18 INFO spark.SparkContext: Starting job: top at <console>:20
15/10/15 23:37:18 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Registering RDD 5 (map at <console>:18)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Got job 1 (top at <console>:20) with 2 output partitions (allowLocal=false)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Final stage: Stage 4(top at <console>:20)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 3)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Missing parents: List(Stage 3)
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Submitting Stage 3 (MappedRDD[5] at map at <console>:18), which has no missing parents
15/10/15 23:37:18 INFO storage.MemoryStore: ensureFreeSpace(6472) called with curMem=206010, maxMem=278302556
15/10/15 23:37:18 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.3 KB, free 265.2 MB)
15/10/15 23:37:18 INFO storage.MemoryStore: ensureFreeSpace(4239) called with curMem=212482, maxMem=278302556
15/10/15 23:37:18 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.2 MB)
15/10/15 23:37:18 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:58634 (size: 4.1 KB, free: 265.4 MB)
15/10/15 23:37:18 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/10/15 23:37:18 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/10/15 23:37:18 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 3 (MappedRDD[5] at map at <console>:18)
15/10/15 23:37:18 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
15/10/15 23:37:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/15 23:37:18 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, localhost, PROCESS_LOCAL, 1045 bytes)
15/10/15 23:37:18 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 4)
15/10/15 23:37:18 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 5)
15/10/15 23:37:18 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/15 23:37:18 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/15 23:37:20 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1001 bytes result sent to driver
15/10/15 23:37:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 1800 ms on localhost (1/2)
15/10/15 23:37:20 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1001 bytes result sent to driver
15/10/15 23:37:20 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 1804 ms on localhost (2/2)
15/10/15 23:37:20 INFO scheduler.DAGScheduler: Stage 3 (map at <console>:18) finished in 1.806 s
15/10/15 23:37:20 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/10/15 23:37:20 INFO scheduler.DAGScheduler: running: Set()
15/10/15 23:37:20 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/10/15 23:37:20 INFO scheduler.DAGScheduler: waiting: Set(Stage 4)
15/10/15 23:37:20 INFO scheduler.DAGScheduler: failed: Set()
15/10/15 23:37:20 INFO scheduler.DAGScheduler: Missing parents for Stage 4: List()
15/10/15 23:37:20 INFO scheduler.DAGScheduler: Submitting Stage 4 (MapPartitionsRDD[10] at top at <console>:20), which is now runnable
15/10/15 23:37:20 INFO storage.MemoryStore: ensureFreeSpace(3376) called with curMem=216721, maxMem=278302556
15/10/15 23:37:20 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 265.2 MB)
15/10/15 23:37:20 INFO storage.MemoryStore: ensureFreeSpace(2395) called with curMem=220097, maxMem=278302556
15/10/15 23:37:20 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 265.2 MB)
15/10/15 23:37:20 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:58634 (size: 2.3 KB, free: 265.4 MB)
15/10/15 23:37:20 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/10/15 23:37:20 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:838
15/10/15 23:37:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 4 (MapPartitionsRDD[10] at top at <console>:20)
15/10/15 23:37:20 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
15/10/15 23:37:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/15 23:37:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, localhost, PROCESS_LOCAL, 1056 bytes)
15/10/15 23:37:20 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 6)
15/10/15 23:37:20 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/10/15 23:37:20 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 7)
15/10/15 23:37:20 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/10/15 23:37:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/10/15 23:37:21 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 6). 1833 bytes result sent to driver
15/10/15 23:37:21 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 7). 1833 bytes result sent to driver
15/10/15 23:37:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 360 ms on localhost (1/2)
15/10/15 23:37:21 INFO scheduler.DAGScheduler: Stage 4 (top at <console>:20) finished in 0.363 s
15/10/15 23:37:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 362 ms on localhost (2/2)
15/10/15 23:37:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/10/15 23:37:21 INFO scheduler.DAGScheduler: Job 1 finished: top at <console>:20, took 2.221294 s
(5.0,zx1lLUvlRUN5nQlSj3HRDw)
(5.0,zweAzZDJ8SfwUID1UEGkbA)
(5.0,zvo21oKr656PQNVblYxYlg)
(5.0,ztswDToyZGyL_dxo0CEQew)
(5.0,zrmO-d-Mw3kv9dWa7Trr9Q)
(5.0,zqZjrcTf9Bc7r_VLGN4mrw)
(5.0,zkD9AtBT9ZkLFiV31gvEGw)
(5.0,ziS_fZ7Z99fa4qNVr879vg)
(5.0,zh2Eja5h54vM7GUibHoi9A)
(5.0,zh-MNFY4TyG7x2itUVlESQ)
finalOP: Unit = ()

scala> 
